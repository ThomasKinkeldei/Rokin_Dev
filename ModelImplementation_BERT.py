# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19H2OeFUJpf7lN_4LRs_6Zs-IMn2ULfdJ
"""

!pip install -U sentence-transformers

import pandas as pd
from tqdm import tqdm

path = r'processed.csv' # use your path
# all_files = glob.glob(path + "/*.csv")

# df = pd.read_csv('data_searchAPI/output.csv')
# li = []

# for filename in all_files:
#    df = pd.read_csv(filename, index_col=None, header=0)
#    li.append(df)

# frame = pd.concat(li, axis=0, ignore_index=True)

# frame.head()

df = pd.read_csv(path, dtype={
    'date': str, 
    'keyword': str
})
#df.columns=['title', 'domain', 'url', 'snippet', 'keyword', 'date']
df.head()

print(df.url.duplicated().sum())
print(df.snippet.isnull().sum())

df.drop_duplicates(keep='first')

df['snippet'].fillna('no description', inplace=True)
df['title'].fillna('no title', inplace=True)
df.info()

#df.dropna(inplace=False)

import re

df['snippet'] = df['snippet'].apply(lambda x: re.sub('[^a-zA-z0-9\s]','',x))

df.head(114)

df = df.drop(["date"], axis=1)

df.head()

df.keyword.isnull().sum()

df_dict = df.to_dict()
len_text = len(df_dict["keyword"])

len_text

keyword_list  = [] # keyword
snippet_description_list = [] # snippet_description
snippet_list = [] # snippet
title_list = [] # title
url_list = [] # url 
domain_list = [] # domain
for i in tqdm(range(0,len_text)):
  keyword = df_dict["keyword"][i]
  snippet = df_dict["snippet"][i].split("\n")
  snippet_description = df_dict["snippet"][i]
  title = df_dict["title"][i]
  url = df_dict["url"][i]
  domain = df_dict["domain"][i]
  for b in snippet:
    keyword_list.append(keyword)
    snippet_list.append(b)
    snippet_description_list.append(snippet)
    title_list.append(title)
    url_list.append(url)
    domain_list.append(domain)

df_sentences = pd.DataFrame({"keyword":keyword_list},index=snippet_list)
df_sentences.to_csv("keyword.csv")
df_sentences.head()

df_sentences = pd.DataFrame({"keyword":keyword_list,"title":title_list,"url":url_list,"domain":domain_list, "snippet_description": snippet_description_list },index=snippet_list)
df_sentences.to_csv("Processed_Full.csv")
df_sentences.head()

df_sentences = pd.read_csv("keyword.csv")
df_sentences = df_sentences.set_index("Unnamed: 0")
df_sentences.head()

df_sentences = df_sentences["keyword"].to_dict()
df_sentences_list = list(df_sentences.keys())
len(df_sentences_list)

list(df_sentences.keys())[:5]

df_sentences_list = [str(d) for d in tqdm(df_sentences_list)]

import pandas as pd
df = pd.read_csv("Processed_Full.csv", index_col=0)
df.head()

#https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py
"""
This is a simple application for sentence embeddings: semantic search
We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.
This script outputs for various queries the top 5 most similar sentences in the corpus.
"""

from sentence_transformers import SentenceTransformer
import scipy.spatial
import pickle as pkl
embedder = SentenceTransformer('bert-base-nli-mean-tokens')

# Corpus with example sentences
corpus = df_sentences_list
corpus_embeddings = embedder.encode(corpus,show_progress_bar=True)
#with open("Processed_Full.csv" , "rb") as file_:
#  corpus_embeddings = pd.read_csv(file_)

# Query sentences:
queries = ['reduction in production costs injection molding']
query_embeddings = embedder.encode(queries,show_progress_bar=True)

# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
closest_n = 5
print("\nTop 5 most similar sentences in corpus:")
for query, query_embedding in zip(queries, query_embeddings):
    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, "cosine")[0]

    results = zip(range(len(distances)), distances)
    results = sorted(results, key=lambda x: x[1])

    print("\n\n=========================================================")
    print("==========================Query==============================")
    print("===",query,"=====")
    print("=========================================================")


    for idx, distance in results[0:closest_n]:
        print("Score:   ", "(Score: %.4f)" % (1-distance) , "\n" )
        print("Paragraph:   ", corpus[idx].strip(), "\n" )
        row_dict = df.loc[df.index== corpus[idx]].to_dict()
        print("Keyword:  " , row_dict["keyword"][corpus[idx]] , "\n")
        print("Title:  " , row_dict["title"][corpus[idx]] , "\n")
        print("Domain:  " , row_dict["domain"][corpus[idx]] , "\n")
        print("Url:  " , row_dict["url"][corpus[idx]] , "\n")
        print("-------------------------------------------")

